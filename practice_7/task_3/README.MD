# Задание 7.3 — Анализ производительности CUDA

## Цель работы  
Целью данной работы является анализ производительности операций **редукции (суммирования)** и **префиксной суммы (scan)** для массивов разного размера.  
В ходе работы сравниваются реализации на **CPU** и **GPU**, а также оценивается влияние использования различных типов памяти и алгоритмов CUDA.

---

## Что реализовано в работе  
В программе реализованы и протестированы следующие варианты:

**Редукция (sum):**
- CPU-реализация (последовательная).
- GPU-реализация с использованием `atomicAdd` в глобальной памяти.
- GPU-реализация с использованием **shared memory** и многошаговой редукции.

**Префиксная сумма (scan):**
- CPU-реализация (последовательная).
- GPU-реализация с алгоритмом **Hillis–Steele** (shared memory).
- GPU-реализация с алгоритмом **Blelloch** (shared memory).

Также измеряется погрешность результатов GPU по сравнению с CPU.

---

## Тестовые данные  
Тестирование проводилось на массивах случайных чисел следующих размеров:
- `1024`
- `1 000 000`
- `10 000 000`

Все измерения времени выполнены в миллисекундах (ms).

---

## Результаты экспериментов  

### Размер массива N = 1024
- CPU reduction time: **0.002946 ms**  
- CPU scan time: **0.001424 ms**

- GPU reduction (atomic): **0.176064 ms**, abs error = **0.000197**  
- GPU reduction (shared): **0.026944 ms**, abs error = **0.000017**

- GPU scan Hillis–Steele: **0.058688 ms**  
- GPU scan Blelloch: **0.052608 ms**, max abs error = **0.000031**

Для малого массива CPU работает быстрее из-за накладных расходов на запуск CUDA-ядер.

---

### Размер массива N = 1 000 000
- CPU reduction time: **2.886645 ms**  
- CPU scan time: **1.362466 ms**

- GPU reduction (atomic): **3.522336 ms**, abs error = **1.494771**  
- GPU reduction (shared): **0.071584 ms**, abs error = **0.036479**

- GPU scan Hillis–Steele: **0.196960 ms**  
- GPU scan Blelloch: **0.397888 ms**, max abs error = **0.062500**

Для массивов среднего размера GPU с shared memory значительно быстрее CPU.  
Использование `atomicAdd` показывает низкую производительность и большую погрешность.

---

### Размер массива N = 10 000 000
- CPU reduction time: **28.802647 ms**  
- CPU scan time: **14.145180 ms**

- GPU reduction (atomic): **35.110401 ms**, abs error = **160.074547**  
- GPU reduction (shared): **0.519680 ms**, abs error = **0.574547**

- GPU scan Hillis–Steele: **1.479552 ms**  
- GPU scan Blelloch: **3.469344 ms**, max abs error = **1.000000**

Для больших массивов GPU показывает существенное преимущество по времени.  
Редукция через shared memory ускоряется более чем на порядок по сравнению с CPU.

---

## Анализ результатов  
- GPU-реализация редукции с использованием **shared memory** значительно быстрее как CPU, так и варианта с `atomicAdd`.
- Использование `atomicAdd` в глобальной памяти приводит к:
  - высокой задержке,
  - плохой масштабируемости,
  - заметной численной погрешности.
- Для операции scan алгоритм **Hillis–Steele** показывает лучшую производительность на больших массивах по сравнению с Blelloch в данной реализации.
- Абсолютная погрешность растёт с увеличением размера массива, что связано с:
  - использованием типа `float`,
  - разным порядком выполнения операций сложения на CPU и GPU.

---

## Вывод  
В ходе работы было показано, что использование **shared memory** и правильно подобранных алгоритмов CUDA существенно повышает производительность операций редукции и префиксной суммы.  
GPU особенно эффективен при работе с большими объёмами данных, тогда как для малых массивов CPU может быть быстрее из-за меньших накладных расходов.
