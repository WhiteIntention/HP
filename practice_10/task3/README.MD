# Задание 3. Профилирование гибридного приложения CPU + GPU (CUDA)
В данной работе реализовано гибридное приложение, где обработка массива выполняется одновременно на CPU и GPU. Для GPU используются **асинхронные передачи данных** (`cudaMemcpyAsync`) и **CUDA streams**, а также проводится профилирование, чтобы оценить накладные расходы и найти узкие места.

## Операция обработки
Для всех элементов массива выполняется одинаковая операция: out[i] = in[i] * k + 1

Параметры запуска:
- `N = 10 000 000`
- `k = 3.5`
- Разделение (split): первая половина на CPU, вторая половина на GPU  
  - CPU: `5 000 000` элементов  
  - GPU: `5 000 000` элементов  

## Реализация гибридного алгоритма
1. **CPU часть** обрабатывает первую половину массива в отдельном потоке (`std::thread`).
2. **GPU часть** обрабатывает вторую половину массива в CUDA stream:
   - `H2D` копирование (Host → Device) через `cudaMemcpyAsync`
   - запуск CUDA-ядра
   - `D2H` копирование (Device → Host) через `cudaMemcpyAsync`
3. В конце выполняется синхронизация stream и объединение результата в общий выходной массив.

## Профилирование и метрики
Для профилирования использованы:
- `cudaEvent` — для измерения времени передачи и выполнения GPU-ядра,
- `std::chrono` — для измерения времени CPU-части и общего времени гибридного алгоритма.

Измеряются:
- **CPU part time** — время обработки CPU-части;
- **GPU H2D time** — накладные расходы на передачу данных на GPU;
- **GPU kernel time** — чистое время выполнения ядра;
- **GPU D2H time** — накладные расходы на копирование результата обратно;
- **Transfer overhead** = H2D + D2H — суммарные накладные расходы передачи;
- **GPU pipeline** = H2D + kernel + D2H — полная GPU-цепочка;
- **Total hybrid time** — итоговое время гибридного запуска.

## Результаты профилирования
Фактический вывод программы:
CPU part time (ms) = 4.044291
GPU H2D time (ms) = 1.701120
GPU kernel time (ms) = 0.305696
GPU D2H time (ms) = 1.544256
Transfer overhead (H2D+D2H) (ms) = 3.245376
GPU pipeline (H2D+K+D2H) (ms) = 3.551072
Total hybrid time (ms) = 4.044454
Overlap estimate (ms) = 0.000163
Correct = YES

## Накладные расходы передачи данных (п.3a)
Накладные расходы передачи данных составили:
- `H2D + D2H = 3.245376 ms`
При этом время вычислений ядра намного меньше:
- `kernel = 0.305696 ms`
Это означает, что в данной задаче основное время на GPU связано не с вычислениями, а с передачей данных между CPU и GPU.

## Узкие места (п.3b)
Главное узкое место — **передача данных** (H2D/D2H), так как она занимает большую часть GPU pipeline.  
Также видно, что перекрытие CPU и GPU выполнено почти полностью:
- `Total hybrid time ≈ CPU part time`
Оценка перекрытия:
- `Overlap estimate = 0.000163 ms` (практически ноль)
Это показывает, что общая длительность гибридного запуска определяется наиболее долгой веткой — в данном случае CPU частью.

## Оптимизация (п.4)
В качестве оптимизации реализовано использование **pinned memory** (`cudaHostAlloc`). Это уменьшает накладные расходы передачи данных и позволяет корректно использовать асинхронные копирования в `cudaMemcpyAsync`, повышая эффективность взаимодействия CPU и GPU.

## Проверка корректности
Результаты совпадают с эталоном:
- `Max abs error = 0`
- `Correct = YES`

## Вывод
- Асинхронная передача данных и CUDA streams позволяют организовать параллельную работу CPU и GPU.
- Для поэлементной операции вычисления на GPU очень быстрые, но значительную часть времени занимают передачи данных.
- Накладные расходы H2D/D2H являются ключевым ограничением производительности гибридных приложений.
- Использование pinned memory снижает стоимость копирования и улучшает эффективность передачи данных.
