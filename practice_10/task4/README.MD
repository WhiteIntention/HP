# Задание 4. Анализ масштабируемости распределённой программы (MPI)
В данной работе исследуется масштабируемость распределённой программы на **MPI**, предназначенной для вычисления агрегатных функций (сумма, минимум, максимум) над большим массивом данных. Основное внимание уделяется влиянию числа процессов и коммуникационных операций на производительность.

## Цель работы
- реализовать распределённую MPI-программу для агрегатных вычислений;
- измерить время выполнения при различном числе процессов;
- оценить **strong scaling** и **weak scaling**;
- проанализировать влияние коллективных операций `MPI_Reduce` и `MPI_Allreduce`;
- сделать выводы о масштабируемости и практических ограничениях алгоритма.

## Описание реализации
Каждый MPI-процесс:
1. локально генерирует свою часть массива данных;
2. вычисляет локальные агрегаты (сумма, минимум, максимум);
3. участвует в коллективной операции:
   - `MPI_Reduce` — сбор результата на одном процессе;
   - `MPI_Allreduce` — распространение результата на все процессы.
Временные замеры выполняются с использованием `MPI_Wtime()`.  
Для корректной оценки масштабируемости анализируется **максимальное время среди всех процессов**.

## Strong scaling
В режиме **strong scaling** общий размер данных фиксирован, а число процессов увеличивается.
Наблюдения показывают, что:
- при малом числе процессов основную часть времени занимают вычисления;
- с ростом числа процессов доля времени, затрачиваемого на коммуникации, существенно возрастает;
- ускорение ограничивается накладными расходами на коллективные операции.

## Weak scaling
В режиме **weak scaling** размер данных на один процесс фиксирован, а общий объём данных растёт пропорционально числу процессов.
Результаты показывают, что:
- вычислительная часть остаётся примерно стабильной;
- время коммуникаций растёт с увеличением числа процессов;
- общее время выполнения увеличивается, что указывает на ограниченную масштабируемость.

## Влияние коммуникационных операций
- **MPI_Reduce** имеет меньшие накладные расходы, так как результат требуется только одному процессу.
- **MPI_Allreduce** требует дополнительных обменов данными, поэтому обладает более высокой стоимостью.
- При увеличении числа процессов именно коллективные операции становятся основным узким местом программы.

## Вывод
- MPI-программа эффективно масштабируется только при ограниченном числе процессов.
- Коммуникационные операции существенно ограничивают как strong scaling, так и weak scaling.
- Использование `MPI_Allreduce` снижает масштабируемость по сравнению с `MPI_Reduce`.
- Для практического применения распределённых вычислений необходимо учитывать баланс между вычислениями и обменом данными.

## Используемые технологии
- **C++**
- **MPI (OpenMPI)**
- `MPI_Wtime()` для профилирования времени выполнения
