# Assignment 4 - Task 4 (25 pts)
## Распределённая обработка массива с использованием MPI

В этом задании реализована **распределённая программа на MPI**, в которой массив данных:
1. **разделяется между процессами**,
2. **обрабатывается локально** каждым процессом,
3. **собирается обратно** на корневом процессе (rank 0).

Проведены замеры времени выполнения для **2, 4 и 8 процессов**.

## Операция обработки
Для каждого элемента массива выполняется одинаковая операция: out[i] = in[i] * k + 1
где:
- `in[]` - входной массив,
- `out[]` - выходной массив,
- `k = 3.5`.

## Используемый подход MPI
- **MPI_Scatterv** - распределение частей массива между процессами  
- **Локальные вычисления** - каждый процесс обрабатывает только свой фрагмент  
- **MPI_Gatherv** - сбор результатов обратно на root-процессе  
- **MPI_Wtime** - измерение общего времени выполнения  

Разбиение массива учитывает случай, когда `N` не кратно числу процессов.

## Параметры эксперимента
- Размер массива: `N = 1 000 000`
- Коэффициент: `k = 3.5`
- Число процессов: **2, 4, 8**

## Результаты измерений
Фактический вывод программы:
### Processes = 2
Total time (ms) = 4.738793
Correct = YES
### Processes = 4
Total time (ms) = 5.042139
Correct = YES
### Processes = 8
Total time (ms) = 9.860996
Correct = YES

## Сводная таблица
| Число процессов | Время (ms) | Относительное ускорение* |
|----------------:|-----------:|--------------------------:|
| 2               | 4.738793   | 1.00×                     |
| 4               | 5.042139   | 0.94×                     |
| 8               | 9.860996   | 0.48×                     |

\* Ускорение считается относительно варианта с 2 процессами.

## Анализ результатов
- Корректность вычислений подтверждена во всех запусках:
  - `Max abs error = 0`
  - `Correct = YES`
- При увеличении числа процессов **время выполнения растёт**, а не уменьшается.

### Причины:
1. **Накладные расходы MPI**  
   - `MPI_Scatterv` и `MPI_Gatherv` требуют пересылки данных между процессами.
2. **Один вычислительный узел (Colab)**  
   - Все процессы конкурируют за одни и те же CPU-ресурсы.
3. **Использование oversubscribe**  
   - Число MPI-процессов превышает количество доступных ядер.
4. **Слишком простая операция**  
   - Вычисления (`in[i] * k + 1`) очень лёгкие и не компенсируют коммуникационные издержки.

## Вывод
Распределённая MPI-программа корректно реализует разделение данных, локальные вычисления и сбор результатов.  
Однако в условиях одного вычислительного узла и простой операции увеличение числа процессов **не приводит к ускорению**, а наоборот увеличивает время выполнения из-за накладных расходов на межпроцессное взаимодействие.
Данный результат является ожидаемым и хорошо иллюстрирует, что **MPI эффективно масштабируется только при достаточной вычислительной нагрузке и наличии нескольких физических узлов**.
